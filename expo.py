from shapely.geometry import Polygon, LineString
from read_data import read_tunnels,read_BP,read_SIREN,read_build_stats
from collections import defaultdict, Counter
from os.path import isfile,isdir,join
from os import listdir,makedirs
from datetime import datetime, timedelta
from bisect import bisect_left
from random import uniform
from multiprocessing import Process as prll
from multiprocessing import Pool
from dateutil.easter import *
from collections import Counter as cnt
from numpy import searchsorted as srch
import cPickle,fnmatch,sys,visual,glob
import numpy as np

def exposure(conc_dir,out_dir,samp_dir,dom,period_exp,procc,idf_cells,building_stock,conc_period,chimere_exp,static):
  
    print 'preparing calculation... please wait...'; print ''; threads=[]; pp_file=samp_dir+'/profiles.dat'; IO_file='IO_indoor.dat'; stk_str=['','_STATIC']; bld_npg=''
    IOB_f='_C'+str(conc_period)+'_B'+str(building_stock); base=[0,0,0,0]; build_type={1:0,4:1,6:1,7:2}; ageB_type={0:8,1:9,2:9} 

    try: st=open('domains/'+str(dom)+'/COORDS_'+str(dom)+'.dat','rb'); grid_coords=cPickle.load(st); st.close()
    except: 
       print 'the file domains/'+str(dom)+'/COORDS_'+str(dom)+'.dat was not found.' 
       print 'This file is generated by the concentrations pre-processor. Please run this module whithout activating the diary statistics pre-processor.'; sys.exit(1)
  
    if int(building_stock)>2008 and not isfile(samp_dir+'/profiles_'+str(building_stock)+'.dat'): update_prof(int(building_stock),samp_dir); print 'updating profiles for building stock evolution...'; print ''
    if int(building_stock)>2008: pp_file=samp_dir+'/profiles_'+str(building_stock)+'.dat'; out_dir+='_BLD'+str(building_stock); bld_npg='_BLD'+str(building_stock); base[0]=1
    if int(conc_period)>2008: IO_file='IO_indoor_'+str(conc_period)+'.dat'; base[1]=1
    if static==1: out_dir+='_STATIC'; base[2]=1    
                        
    try: st=open('data/IO_SIREN/'+IO_file,'rb'); IO_indoor=cPickle.load(st); st.close()
    except: IO_indoor=read_SIREN(conc_period); visual.plot_SIREN(IO_indoor,conc_period) # IO_indoor is pols,months,BT,ageB,bins
    IO_seas=get_IOB_for_pop(IO_indoor) # i create a seasonal I/O database
    
    for d in [i for i in [75,77,78,91,92,93,94,95] if chimere_exp==1]: IO_indoor[d]=np.zeros((2,12,3,5,21)); base[3]=1
    dt_procc=get_start_end_date(conc_dir,period_exp,procc,out_dir,dom); BP=read_BP(); dx,dy=get_grid_dims(grid_coords); st=open('domains/'+dom+'/grid_polygons.dat','rb')
    grid_polyg=cPickle.load(st); st.close(); BP_cells,paris_cells=BP_intersections(BP,grid_polyg); st=open(pp_file,'rb'); pp=cPickle.load(st); st.close(); nb=len(pp)
                
    try: st=open('domains/'+dom+'/tunnels.dat','rb'); tunnels=cPickle.load(st); st.close()
    except: 
        tunnel_coords=read_tunnels(dom); tunnels=[] # this is the array with all the grid cells (they start from 1!) that have a tunnel       
        for tn in tunnel_coords.keys():     
            lat1=float(tunnel_coords[tn][1]);lon1=float(tunnel_coords[tn][2]);lat2=float(tunnel_coords[tn][3]);lon2=float(tunnel_coords[tn][3]);tmp=[[lon1,lat1],[lon2,lat2]];tun=LineString(tmp)                            
            for cell in [i for i in grid_polyg.keys() if tun.intersects(grid_polyg[i][0]) and i not in tunnels]: tunnels.append(cell)
        f=open('domains/'+dom+'/tunnels.dat','wb'); cPickle.dump(tunnels,f,-1); f.close() 
            
    if not isdir(out_dir): makedirs(out_dir)
    
    # get the time spend in the activity and current location for all activities of all hours of that individual:      
    print ''; print 'caching diaries...'; print ''; input=np.empty((2,nb,24,3,12)); vc=[[[] for _ in xrange(nb)] for w in xrange(2)]; traj=[[],[]]
    for tp in 'wday wend'.split():    
        try: st=samp_dir+'/trajectories_'+tp+'_'+dom+stk_str[static]+'.dat'; exec("traj_%s=cPickle.load(open(st,'rb'))" %tp)
        except: print 'the file '+st+' is missing, maybe you have to run the diaries for the '+dom+' domain.'; sys.exit(0) 
        try: st=samp_dir+'/input_expo_'+tp+stk_str[static]+'.dat'; exec("time_%s=cPickle.load(open(st,'rb'))" %tp)
        except: print 'the file '+st+' is missing, maybe you have to run the diaries for the '+dom+' domain.'; sys.exit(0)             
        
    for tp in [i for i in 'wday wend'.split() if not isfile(samp_dir+'/npg_'+i+bld_npg+'.dat')]:
        print 'compiling groups database '+tp; exec("dr=time_%s" %tp); ipg=get_groups(nb,pp,dr); f=open(samp_dir+'/npg_'+tp+bld_npg+'.dat','wb'); cPickle.dump(ipg,f,-1); f.close(); print ''

    if len(glob.glob(samp_dir+'/*_REF.dat'))<>4 and sum(base)<>4: ref_exposure(conc_dir,np.hstack(dt_procc),time_wday,samp_dir,nb,dx*dy)   
        
    for w,tp in enumerate('wday wend'.split()):
        exec("tm=time_%s" %(tp)); exec("traj[w]=traj_%s" %(tp)); exec("traj_%s=[]" %(tp))                
        for k in xrange(nb):                                                
            vc[w][k]=tm[k][24]; put=[0,0,0,0,0,0,0,0,0,0,0]                        
            for h in xrange(24): # we loop each hour of the individual's diaries      				
                for a in xrange(len(tm[k][h])): # all the different activities performed in this hour
                
                    # put[..] is time spend, curr_cell, dest_cell, IO_case, transport IO, curr_loc, dest_loc, motive                                                         
                    put[0],put[1],put[2],put[3],read_IO,put[5],put[6],put[7]=tm[k][h][a].split(','); put[8]=len(tm[k][h]) # put[8] is the max acts per hour
                    if int(put[3]) in [1,4,6,7]: BT=build_type[int(put[3])]; ageB=pp[k][ageB_type[BT]]-1; put[9]=BT; put[10]=ageB
                                
                    for indx,i in enumerate([0,1,2,3,5,8,9,10]): input[w,k,h,a,indx]=put[i]     
                    for indx,i in enumerate(map(float,read_IO.split('/'))): input[w,k,h,a,8+indx]=i # put the I/O ratios for transport
        
    if chimere_exp==1: input[:,:,:,:,8:]=1. # IO ratios are set to 1 for buildings and transport
        
    # calculate exposure in parallel mode
    for indx,dates in enumerate(dt_procc): threads.append(prll(target=calc_exp,args=[input,dates,traj,conc_dir,out_dir,dom,nb,vc,dx,dy,tunnels,BP_cells,paris_cells,idf_cells,indx,IO_seas]))      
    for thread in threads: thread.start()
    for thread in threads: thread.join()
        
def get_IOB_for_pop(IO_indoor):
    
    IO_seas=dict(); max_tp={0:5,1:3,2:3}; s_str={0:[1,2,12],1:[3,4,5],2:[6,7,8],3:[9,10,11]}
    
    for d in [75,77,78,91,92,93,94,95]: # transform the distribution from monthly to seasonal
        IO_seas[d]=np.zeros((2,4,3,5,21)) # pols,seasons,BT,ageB,bins
        for s in range(4):
            for m in s_str[s]: IO_seas[d][:,s,:,:,:]+=IO_indoor[d][:,m-1,:,:,:]            
                   
    IO_indoor.clear() # get the probability arrays instead of counts 
    for p in range(2):        
        for d in [75,77,78,91,92,93,94,95]:
            for BT in range(3):
                for tp in range(max_tp[BT]):
                    for s in range(4): tmp=sum(IO_seas[d][p,s,BT,tp]); IO_seas[d][p,s,BT,tp]/=tmp
    
    return IO_seas
    
def get_IOB(IO_seas,nbs,tm,pp,indx): # tm here has the IO_case, curr_loc and number of acts
        
    IOB_tmp=np.zeros((len(pp),24,3,8)); build_type={1:0,4:1,6:1,7:2}; ageB_type={0:8,1:9,2:9} # other establisments get the IORs of offices
    if indx==0: previous=0; done=0
        
    for k in nbs:
        for h in xrange(24): # we loop each hour of the individual's diaries    
            acts=int(tm[k,h,0,-1])                                                               
                
            for a in xrange(acts): # all the different activities performed in this hour                                   
                case,curr_dep=int(tm[k,h,a,0]),int(int(tm[k,h,a,1])/1000.)
                                
                for p in [i for i in range(2) if case in [1,4,6,7]]: # case1: home, case7: school, case4: outdoors market/fun, case6: work/prof  
                    
                        
                    for s in range(4): perc_IO=IO_seas[curr_dep][p,s,BT,ageB].tolist(); id=selection(perc_IO,0); IOB_tmp[k,h,a,s+4*p]=float(id)*0.05+0.025
                    
        if indx==0: done+=1.; previous=get_progress_bar(done,len(nbs),previous)             
        
    return IOB_tmp
  
def update_prof(building_stock_sce,samp_dir):

    import matplotlib
    import pylab as plt

    # CSTB future scenario for buildings
    # houses: demolishion 1%, rehab cs1-->cs2 1.12%, cs1-->cs2 0.38%, new 1.2%
    # tertiary: demolision 3%, rehab cs1-->cs2 0%, cs1-->cs2 0%, new 3%
    
    build=read_build_stats(); st=open(samp_dir+'/profiles.dat','rb'); pp=cPickle.load(st); st.close(); prob=dict(); c=[[0,0,0,0,0],[0,0,0]]; f=open(samp_dir+'/building_evol.txt','w')
    cl=['g','r','b','yellow','orange']; lg=[['cat1','cat2','cat3','cat4','cat5'],['cat1','cat2','cat3']]; cl1=[[cl[1],cl[4],cl[3],cl[2],cl[0]],[cl[1],cl[3],cl[2]]]; inv=[3,2]
    b_str=['house','tetriary']; chg=[[1.,1.12,0.38,1.2],[3.,0.,0.,3.]]
    
    def get_build(curr_bld,rate,yr,b):
        
        stock=sum(curr_bld); rate_demol=rate[0]/100.; rate_rehab1=rate[1]/100.; rate_rehab2=rate[2]/100.; rate_new=rate[3]/100.
                
        # first we destroy old buildings; buildings demolished are removed from the pre-1974 buildings (cs1)
        cs1=curr_bld[0]; demol=int(stock*rate_demol); demol=min([demol,cs1]); cs1=curr_bld[0]-demol; curr_bld[0]=cs1
        
        # then we get the rehabilitated buildings; 75% of those are removed from cs1 and 25% from cs2. the rehab buildings move one category up (cs1 to cs2 and cs2 to cs3)
        cs2=curr_bld[1]; rehab_cs1=int(stock*rate_rehab1); rehab_cs2=int(stock*rate_rehab2); rehab_cs1=min([rehab_cs1,cs1]); rehab_cs2=min([rehab_cs2,cs2])
        cs1=curr_bld[0]-rehab_cs1; cs2=curr_bld[1]-rehab_cs2+rehab_cs1; cs3=curr_bld[2]+rehab_cs2; curr_bld[0]=cs1; curr_bld[1]=cs2; curr_bld[2]=cs3
                        
        # finally based on the remaining stock i calculate the new constructions; these are added to the last category (cs5 for houses, cs3 for offices/schools)
        new=int(stock*rate_new); curr_bld[-1]+=new; curr_bld=[int(i) for i in curr_bld]; flux=np.array([demol,rehab_cs1,rehab_cs2]); return flux,curr_bld
        
    for d in [75,77,78,91,92,93,94,95]: # get the evolution of buildings from 2008 to the future year
        prob[d]=[[[],[]],[[],[]]]; print >> f,''; print >> f,str(d)
        
        for b in [0,1]: # 0: houses, 1: tetriary
            bld_new=build[d][b]; flux=np.array([0.,0.,0.]); tmp=np.array([build[d][b][0],build[d][b][0],build[d][b][1]]); print >> f,''; print >> f,b_str[b]
            for yr in range(2009,building_stock_sce+1): 
            
                flux_out,bld_new=get_build(bld_new,chg[b],yr,b); flux+=flux_out; wrt=['---'+str(yr)+'---']+bld_new+[str(sum(bld_new))]
                                
                if b==0: line = '{:<15} {:<15} {:<15} {:<15} {:<15} {:<15} {:<15}'.format(*wrt); print >> f,line
                else: line = '{:<15} {:<15} {:<15} {:<15} {:<15}'.format(*wrt); print >> f,line
                                        
            # this represents the probabilities for a house to be demolished, rehabilitated from cs1 to cs2 or rehabilitated from cs2 to cs3
            flux[-1]=min([flux[-1],tmp[-1]]); perc=flux/tmp; prob[d][b][0]=[perc[0],perc[1]]; prob[d][b][1]=[perc[2]]
            print >>f,'prob.','dem='+str(round(perc[0],2)),'cs1cs2='+str(round(perc[1],2)),'cs2cs3='+str(round(perc[2],2))
                    
    for k in xrange(len(pp)): # now i loop all individuals
        cs=[pp[k][8],pp[k][9]]; prb=prob[int(int(pp[k][0])/1000.)]
        
        for b in [0,1]: # houses / offices and schools
                
            if cs[b]>=inv[b]: continue # houses with cs>3 or offices with case>2 are not valid for movement
            else: # the person is eligible for moving to a new building
              
              if b==0: # houses
                 p=prb[b][cs[0]-1]; id=selection(p,0)
                 if cs[0]==1 and id==0: pp[k][8]=5 # individual in a case1 moves to new construction          
                 elif cs[0]==1 and id==1: pp[k][8]=2 # individual in a case1 with building is rehabilitated, goes to case2                                        
                 elif cs[0]==2 and id==0: pp[k][8]=3 # individual in a case2 with building is rehabilitated, goes to case3
              else: p=[prb[b][0][0]]; id=selection(p,0); pp[k][9]=3-2*id # offices/schools
                                                   
        ha=pp[k][8]-1; c[0][ha]+=1; oa=pp[k][9]-1; c[1][oa]+=1 # count people living in house/office per category                                                     
    
    f=open(samp_dir+'/profiles_'+str(building_stock_sce)+'.dat','wb'); cPickle.dump(pp,f,-1); f.close()

    plt.ioff(); fig=plt.figure(figsize=(45,20)); matplotlib.rcParams['font.size']=60; plt.subplots_adjust(wspace=0.4, hspace=0.2)    
    for indx_bl,bl in enumerate(['houses','offices']): data=np.array(c[indx_bl])/float(len(pp)); ax=fig.add_subplot(1,2,indx_bl+1); ax.pie(data, labels=lg[indx_bl], colors=cl1[indx_bl], autopct='%1.1f%%')                            					
    plt.ion(); fig.savefig(samp_dir+'/building_age_'+str(building_stock_sce)+'.pdf', dpi=100); plt.close(fig)
        
def get_grid_dims(grid_coords):
        
    N=len(grid_coords); lon0=grid_coords[1][0]
    for id_cell in grid_coords.keys():
        if grid_coords[id_cell][0]<lon0: dx=id_cell-1; break
        else: lon0=grid_coords[id_cell][0]
    dy=N/dx
  
    return dx,dy
        
def get_start_end_date(conc_dir,period_exp,procc,out_dir,dom):
        
    dates=[] # first we get all the dates in the conc_dir and select the sub-period based on user definitions of period_exp  
    try:
       flist = sorted([f for f in listdir(conc_dir) if isfile(join(conc_dir,f)) and len(f)==19],key=str.lower)        
       start_date=datetime.strptime(flist[0][5:15],'%Y-%m-%d'); end_date=datetime.strptime(flist[-1][5:15],'%Y-%m-%d'); delta=end_date-start_date             
    except: print 'There are no CHIMERE output files in '+conc_dir; sys.exit(1)
         
    for index_d,d in enumerate(xrange(delta.days+1)): dt=str(start_date+timedelta(days=index_d))[0:10]; dates.append(dt) # all dates found in the conc directory
        
    print 'there are '+str(len(dates))+' day(s) in the '+conc_dir+' directory'
    try: dates=dates[dates.index(period_exp[0]):dates.index(period_exp[1])+1]
    except: print 'ERROR: the start end dates selected for exposure are not found in the concentration output.',dates ; sys.exit(1)
    print 'user has selected a sub-period of '+str(len(dates))+' days ('+dates[0]+' to '+dates[-1]+')'
    dates=[d for d in dates if not isfile(out_dir+'/exp_'+dom+'_'+d+'.dat')]; print 'from that period exposure will be calculated for',str(len(dates)),'days.' # exclude the days already calculated
        
    if len(dates)<=1: print ''; print 'ERROR: at least 2 days are needed for exposure calculation'; sys.exit(1)    
    elif len(dates)<procc: print 'ERROR: the number of days in the selected period cannot be lower than the number of processors'; sys.exit(1)
    
    # then we split the dates in clusters of sub-period based on the number of processors selected for the run
    dates_procc=split_for_multi(procc,dates); return dates_procc  

def split_for_multi(procc,dates):

    dates_procc=[]; chunk=int((len(dates)/float(procc))//1)
                        
    if len(dates)<procc: print 'ERROR: the number of days in the selected period cannot be lower than the number of processors'; sys.exit(1)
    
    for index_d,d in enumerate(dates[0::chunk]): dates_procc.append([]); dates_procc[index_d]=dates[index_d*chunk:(index_d+1)*chunk]
        
    if len(dates_procc)>procc:
       diff=len(dates_procc)-procc; rem=np.hstack(dates_procc[-diff::])  
       for i in range(diff): del dates_procc[-1]
        
       dates_procc.append(rem.tolist())
        
    return dates_procc
    
def BP_intersections(BP,grid_polyg):
    
    paris_cells=[]; BP_cells=[]; points=[]
        
    for c in BP.keys(): points.append(BP[c]) 
    BP_polyg=Polygon(points)

    for cell in [i for i in grid_polyg.keys() if BP_polyg.intersects(grid_polyg[i][0]) and i not in BP_cells]: BP_cells.append(cell) # find which cells intersect with the BP
    for cell in [i for i in grid_polyg.keys() if BP_polyg.contains(grid_polyg[i][0].centroid) and i not in paris_cells]: paris_cells.append(cell) # find which cells are inside Paris
        
    return BP_cells,paris_cells # cells start from 1!

def find_idf_cells(grid_polyg,dom):

    f=open('data/depts_idf.prn','r'); deps=dict(); points=[]; poly_dep=dict(); idf_cells=[]; deps_cells=dict()
        
    for l in f: # get the perimeter points of each department
        if '@' in l:  	   
            if len(points)>0: deps[dp].append(points); points=[]
            dp=l.replace(" ","")[1:3]
            if dp<>'10': deps[dp]=[]     
        else: lon,lat=l.strip().split(' ')[0],l.strip().split(' ')[-1]; tmp=[float(lon),float(lat)]; points.append(tmp)
        
    for d in deps.keys(): poly_dep[d]=[]; poly_dep[d].append(Polygon(deps[d][0])) # create the polygons of each department 
    tmp=[poly_dep[i][0] for i in poly_dep.keys()] 
    
    for cell in grid_polyg.keys(): # cells here start from 1      
          for p in [j for j in tmp if grid_polyg[cell][0].intersects(j)]:         
              if cell not in idf_cells: # i exclude cells with area of less than 30% inside IdF boundaries
                 if (p.intersection(grid_polyg[cell][0]).area/(grid_polyg[cell][0]).area)*100.>30.: idf_cells.append(cell)
                        
    for cell in idf_cells: # cells here start from 1   
        deps_cells[cell]=0.
        for d in deps.keys():
            if grid_polyg[cell][0].intersects(poly_dep[d][0]):
               if ((poly_dep[d][0]).intersection(grid_polyg[cell][0]).area/(grid_polyg[cell][0]).area)*100.>50.: deps_cells[cell]=int(d)
        
    of=open('domains/'+dom+'/idf_cells.dat','wb'); cPickle.dump(idf_cells,of,-1); of.close(); of=open('domains/'+dom+'/deps_cells.dat','wb'); cPickle.dump(deps_cells,of,-1); of.close()
        
def calc_exp(input,dates,traj,conc_dir,out_dir,dom,nb,vc,dx,dy,tunnels,BP_cells,paris_cells,idf_cells,indx_chunk,IO_seas):
        
    # i export a series of categories. each category goes to a specific element of the last dimension of e_tmp which is controlled by the dictionary OT below:
    # INDOOR--> home: case1, work/proff/school: case6/case6/case7, market/recreation/personal: case4
    # OUTDOOR--> market/recreation/on-foot/bicycle/motorbike: case5/case5/case31/case32/case33
    # travelling--> car: case21, bus/RER/metro/tram + waiting time: case22/case23/case24/case25
    OT={1:0,6:1,7:1,4:2,5:3,31:3,32:3,33:3,21:4,22:5,23:5,24:5,25:5}    
    
    season={1:0,2:0,12:0,3:1,4:1,5:1,6:2,7:2,8:2,9:3,10:3,11:3}; wk={0:0,1:0,2:0,3:0,4:0,5:1,6:1}; PT=[22,23,24,25] # static: cases for static, PT: cases for public transport
    previous=0; done=0.; np.seterr(invalid='ignore'); exp=dict(); exp=defaultdict(dict); wkends=get_day_of_week(dates) # gets the weekends and bank holidays    
    if indx_chunk==0: print ''; print 'calculating exposure...'; print '0% completed.'
        
    for indx,day in enumerate(dates): # the period is defined by the available concentration files                         

        if isfile(out_dir+'/exp_'+dom+'_'+day+'.dat') and indx_chunk==0: done+=1.; previous=get_progress_bar(done,len(dates),previous)
        if isfile(out_dir+'/exp_'+dom+'_'+day+'.dat'): continue # it will skip already processed exposure

        w=wk[wkends[indx]]; exec("tm=input[%s]; trv=traj[%s]" %(w,w)); f=open(conc_dir+'/conc_'+day+'.dat','rb'); conc=cPickle.load(f); f.close(); s=season[int(day[5:7])]
       
        for k in xrange(nb):
                
            e_tmp=np.zeros((3,dx*dy,6)) # the dimensions are: o3,pm2.5,time and the 6 exposure categories
            for h in xrange(24): # we loop each hour of the individual's diaries    

                acts=int(tm[k,h,0,5])         
                if h>0: acts_previous=int(tm[k,h-1,0,5]); prev_case=int(tm[k,h-1,acts_previous-1,3])	                
                                
                for a in xrange(acts): # all the different activities performed in this hour (all cells below start from 1 NOT 0!)
                                            
                    wait=0.; time,cc,dc,case,BT,ageB,curr_dep=tm[k,h,a,0],int(tm[k,h,a,1]),int(tm[k,h,a,2]),int(tm[k,h,a,3]),int(tm[k,h,a,6]),int(tm[k,h,a,7]),int(int(tm[k,h,a,4])/1000.); IO=[]                                        
                    if case in PT and prev_case not in PT: wait=min(max(time-4.,0.),4.)
                    p=1; j=0; l1=1; l2=0; time-=wait
                    if case in PT: p=2; l1=2; l2=1
                    
                    # case1=home; case4=market/recreation/personal indoors; case6=work,professional; case7=school                      
                    if case in [1,4,6,7]:                                                                                                                                                          
                       for index_pol in range(2): id=selection(IO_seas[curr_dep][index_pol,s,BT,ageB].tolist(),0); IO.append((float(id)*0.05+0.025))
                                              
                    else: IO=tm[k,h,a,8:12] # transport I/Os
                                                               
                    for index_pol,pol in enumerate('O3 PM25'.split()):
                                                
                        if case==21 and index_pol==1:                        
                           if cc in BP_cells: j=2 # the cell is in the BP
                           elif cc in paris_cells: j=1 # the cell is inside Paris                                               
                                                
                        # the person is static or moves in the same cell (cells start from 1!)                                              
                        if cc==dc: 
                           e_tmp[index_pol,cc-1,OT[case]]+=conc[pol][h][cc-1]*(time*IO[l1*index_pol+l2+j]+wait*IO[p*index_pol])
                           if index_pol==0: 
                              e_tmp[-1,cc-1,OT[case]]+=time+wait                                                                                   

                        else: #the person is travelling in a different cell so we have a path ('Car':21,'Bus':22,'RER':23,'Metro':24,'tram':25,'Foot':31,'Bicycle':32,'Bike':33)
						                               
                           try: sc=int(trv[k][h][a][0]) # the first cell in the list is the starting cell
                           except: trv[k][h][a]=trv[k][h-1][-1]; sc=int(trv[k][h][a][0]) # this cover the rare cases where travelling spans across 3 hours and the last hour is left blank to passing cells.
                              
                           if index_pol==0:
                              tim_cell=time/float(len(trv[k][h][a])) # i assume equal time spend in each cell
                                                           
                           # with PT i add the exposure of the waiting time, the corresponding conc is that of the starting cell (waiting time goes to travelling)
                           if case in PT: 
                              e_tmp[index_pol,sc-1,5]+=wait*conc[pol][h][sc-1]*IO[p*index_pol]
                              if index_pol==0:
                                 e_tmp[-1,sc-1,5]+=wait                                              

                           elif case==21 and index_pol==1: tunnel_p=[0.2]; tun=selection(tunnel_p,0) # i assume a 20% possibility to use a tunnel (arbitrary selection)
                                                                                 
                           for traj_cell in trv[k][h][a]: # cells from 1!
                               IO_final=IO[l1*index_pol+l2]
                                                              
                               if case==21 and index_pol==1:
                                  j=0
                                  if traj_cell in BP_cells: j=2 # the cell is in the BP
                                  elif traj_cell in paris_cells: j=1 # the cell is inside Paris
                                  IO_final=IO[l1*index_pol+l2+j]
                                  
                                  # for PM2.5 an IO ratio of 3 is based on an AP study (Fig. 51 average of 4 tunnels) but they measured number of particles not mass. I use 2.
                                  if traj_cell in tunnels and tun==0: IO_final=IO_final*2.
                                                              
                               e_tmp[index_pol,int(traj_cell)-1,OT[case]]+=tim_cell*conc[pol][h][int(traj_cell)-1]*IO_final
 
                               if index_pol==0:
                                  e_tmp[-1,int(traj_cell)-1,OT[case]]+=tim_cell
                                                
            for c in [i for i in vc[w][k] if i in idf_cells and any(v>0. for v in e_tmp[1,i-1])]:
                exp[c][k]=[[0. for _ in range(6)] for _ in range(3)] # O3,PM2.5,time--> 6 exposure modes
                for j in range(6): exp[c][k][0][j]=int(e_tmp[0,c-1,j]); exp[c][k][1][j]=int(e_tmp[1,c-1,j]); exp[c][k][2][j]=int(e_tmp[-1,c-1,j])
                                
        of=open(out_dir+'/exp_'+dom+'_'+day+'.dat','wb'); cPickle.dump(exp,of,-1); of.close(); exp.clear(); conc.clear()
                
        if indx_chunk==0: done+=1.; previous=get_progress_bar(done,len(dates),previous) # count how many days were completed

def ref_exposure(conc_dir,dates,diary,samp_dir,nb,nb_cells):

    print ''; print 'calculating reference exposure for the first time...'; exp_per_bin=dict(); exp_per_bin=defaultdict(dict); npg_wday=cPickle.load(open(samp_dir+'/npg_wday.dat','rb'))
    exp_per_ind=np.zeros((5,3,nb)); C=np.zeros((5,2,nb_cells)); season={1:0,2:0,12:0,3:1,4:1,5:1,6:2,7:2,8:2,9:3,10:3,11:3}; count=Counter([season[int(day[5:7])] for day in dates])
    pop_groups=['tot','<4','4-24','25-64','>64','P','PC','GC','infants','education','working','unemployed','retired',None,'1974','2005','2012']; home=[]; pol=['o3','pm25']; previous=0; done=0.
    tm=np.zeros((5,2)); home_grd=np.zeros((2,nb_cells),dtype=np.int); out_files=['exp_avg','exp_per_ind','exp_per_bin','exp_per_bin_per_day']
    wkd={0:'wday',1:'wday',2:'wday',3:'wday',4:'wday',5:'wend',6:'wend'}; weekends=get_day_of_week(dates); exp_avg=np.zeros((len(dates),2))
    st=open(samp_dir+'/profiles.dat','rb'); pp=cPickle.load(st); st.close(); exp_per_bin_per_day=[[[] for _ in range(len(dates))] for _ in range(2)]
    
    for k in xrange(nb): # count how many people live in each grid cell 
        A,hc,C1,D,E,F,G,H=diary[k][0][0].split(','); home.append(int(hc)); home_grd[0,int(hc)-1]+=1
        if int(pp[k][0]/1000.)==75: home_grd[1,int(hc)-1]+=1 # Parisians
                
    for s in range(4): tm[s+1]=count[s]*24*60; tm[0]+=tm[s+1]
        
    for indx_d,day in enumerate(dates): # the period is defined by the available concentration files                         
        f=open(conc_dir+'/conc_'+day+'.dat','rb'); conc=cPickle.load(f); f.close(); tmp=np.zeros((2,24,len(conc['O3'][0]))); s=season[int(day[5:7])]; w=wkd[weekends[indx_d]]; indx=[0,0]
        o3=np.array(conc['O3']); pm=np.array(conc['PM25']); tmp[0]=o3; tmp[1]=pm; conc.clear(); Cd=np.sum(tmp,axis=1); C[s+1]+=Cd; C[0]+=Cd             
        
        for g in range(2): # for the total population and the parisians
                       
            dt0=np.repeat(Cd[0]/24.,home_grd[g]); dt1=np.repeat(Cd[1]/24.,home_grd[g]); mx1=int(round(max(dt0),1)/0.1)+1; mx2=int(round(max(dt1),1)/0.1)+1                                             
            cs=[np.array([0.1*i for i in xrange(mx1)]),np.array([0.1*i for i in xrange(mx2)])]; o3,pm25=np.zeros((len(cs[0]))),np.zeros((len(cs[1])))  
                        
            for i,p in enumerate(pol): exec("indx[%s]=srch(cs[%s],dt%s,side='right')-1" %(i,i,i)); exec('%s_=cnt(indx[%s])' %(p,i)); exec('for j in %s_.keys(): %s[j]=%s_[j]' %(p,p,p))                        
            exp_per_bin_per_day[g][indx_d].append(o3); exp_per_bin_per_day[g][indx_d].append(pm25); done+=0.5; previous=get_progress_bar(done,len(dates),previous)                        
                
            if g==0: exp_avg[indx_d,0]=np.mean(dt0); exp_avg[indx_d,1]=np.mean(dt1)
        
    for k in xrange(nb): tmp=C[:,:,home[k]-1]*60.; exp_per_ind[:,[0,1],k]=tmp; exp_per_ind[:,-1,k]=tm[:,-1]
                
    for s in [i for i in range(5) if np.sum(exp_per_ind[i,-1])<>0.]: # annual and then the 4 seasons    
        data=[0,0]; data[0]=exp_per_ind[s,0]/exp_per_ind[s,-1]; data[1]=exp_per_ind[s,1]/exp_per_ind[s,-1]; A=post_output_avg(nb,data,pop_groups,npg_wday) # A gives the bins                           
        for g in pop_groups: exp_per_bin[s][g]=[[],[]]; exp_per_bin[s][g][0]=A[g][0]; exp_per_bin[s][g][1]=A[g][1]           
        
    for fl in out_files: f=open(samp_dir+'/'+fl+'_REF.dat','wb'); exec("cPickle.dump(%s,f,-1); f.close()" %(fl)); exec("del %s" %(fl)) # output the dat files
        
def get_day_of_week(dates):

    weekends=dict(); days_fr=['lundi', 'mardi','mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche']; days=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']; weekends=[]
    years=[]; holidays=[]
    
    for day in dates: # get the unique years within the dates
        if int(day[0:4]) not in years: years.append(int(day[0:4]))    
    
    for y in years: # for each year get the moving bank holidays
        east=easter(y)+timedelta(days=1) # get the date of Easter
        for i in [1,38,49]: d=timedelta(days=i); holidays.append(str(east+d)) # dates of Easter Monday, Ascension and Pentecote
                        
    for indx,day in enumerate(dates):
        ans=datetime.strptime(day,'%Y-%m-%d')
        if ans.strftime("%A") in days_fr: days=days_fr # get the strings in French
        weekends.append(days.index(ans.strftime("%A"))) # ans.strftime("%A") gives e.g. Monday
                  
        if day[5::] in ['01-01','05-01','05-08','07-14','08-15','11-01','11-11','12-25'] or day in holidays: weekends[-1]=6 # bank holidays are treated as Sundays
                                        
    return weekends   
    
def get_statistics(conc_dir,out_dir,samp_dir,dom,ps,procc,idf_cells,path_to_conc,building_stock,chim_label,static):
            
    exp_per_bin=dict(); exp_per_bin=defaultdict(dict); exp_per_cell_per_day=dict(); season={1:0,2:0,12:0,3:1,4:1,5:1,6:2,7:2,8:2,9:3,10:3,11:3}; wk={0:0,1:0,2:0,3:0,4:0,5:1,6:1}; bld_npg=''
    out_files=['exp_flux','exp_avg','exp_per_ind','exp_per_bin_per_day','exp_per_bin','exp_per_cell','exp_per_cell_per_day','exp_per_dep']; procc=int(procc*1.5); res={'P':0,'PC':1,'GC':2}
    pop_groups=['tot','<4','4-24','25-64','>64','P','PC','GC','infants','education','working','unemployed','retired','1974','2005','2012',None]; np.seterr(invalid='ignore')
    reg={92:'PC',93:'PC',94:'PC',77:'GC',78:'GC',91:'GC',95:'GC',75:'P'}; pp_file=samp_dir+'/profiles.dat'; pop_groups1=['tot','P','PC','GC','1974','2005','2012']
    
    st=open('domains/'+dom+'/deps_cells.dat','rb'); deps_cells=cPickle.load(st); st.close(); st=open('domains/'+dom+'/COORDS_'+dom+'.dat','rb'); grid_coords=cPickle.load(st); st.close(); dx,dy=get_grid_dims(grid_coords)
    if int(building_stock)>2008: pp_file=samp_dir+'/profiles_'+str(building_stock)+'.dat'; out_dir+='_BLD'+str(building_stock); bld_npg='_BLD'+str(building_stock)
    if static==1: out_dir+='_STATIC'    
    if not isdir(out_dir): print 'the '+out_dir+' directory was not found.'; sys.exit(1)
     
    st=open(pp_file,'rb'); pp=cPickle.load(st); st.close(); nb=len(pp)
     
    # create date list from the exposure.dat files
    exp_flist=sorted([f for f in listdir(out_dir) if fnmatch.fnmatch(f, 'exp_'+dom+'*.dat')],key=str.lower)
    if exp_flist==[]: print 'no exposure files found in '+out_dir; sys.exit(1)
    flist_all=[]; start_date=datetime.strptime(exp_flist[0][5+len(dom):15+len(dom)],'%Y-%m-%d'); end_date=datetime.strptime(exp_flist[-1][5+len(dom):15+len(dom)],'%Y-%m-%d'); delta=end_date-start_date         
    for index_d,d in enumerate(xrange(delta.days+1)): dt=str(start_date+timedelta(days=index_d))[0:10]; flist_all.append(dt) # all dates found in the output directory         
    if ps[0] not in flist_all or ps[1] not in flist_all: print 'the dates provided in the expo.par file are not in the simulation output ('+flist_all[0]+' to '+flist_all[-1]+') in '+out_dir; sys.exit(0)
    
    print 'there are '+str(len(flist_all))+' exposure day(s) in the '+out_dir+' directory'; dates=flist_all[flist_all.index(ps[0]):flist_all.index(ps[1])+1]; wkends=get_day_of_week(dates)
    print 'user has selected a sub-period of '+str(len(dates))+' days ('+dates[0]+' to '+dates[-1]+')'; #dates=[d for d in dates if not isfile(out_dir+'/daily_avgs/avgs'+d+'.dat')]; 
    print 'from that period post-treatment will be calculated for',str(len(dates)),'days.'; print ''; dates_procc=split_for_multi(procc,dates)

    # gridded average period and seasonal concentrations from the CHIMERE output
    if not isfile(path_to_conc+'/'+chim_label+'/conc_avg_'+dates[0]+'_'+dates[-1]+'.dat'): get_avg_conc(dates,path_to_conc,conc_dir,dx,dy,grid_coords,idf_cells,dom,chim_label); print ''

    # post-processing of the exposure output; for each day we calculate a number of averages (population, gridded etc)
    npg_wday=cPickle.load(open(samp_dir+'/npg_wday'+bld_npg+'.dat','rb')); npg_wend=cPickle.load(open(samp_dir+'/npg_wend'+bld_npg+'.dat','rb')); print ''    
    print 'calculating mean values for each day...'; print ''; pool=Pool(); threads=[]; home=[res[npg_wday['res'][k]] for k in xrange(nb)]
    
    if not isdir(out_dir+'/daily_avgs/'): makedirs(out_dir+'/daily_avgs/')
                
    for indx,dat in enumerate(dates_procc): p=pool.apply_async(calc_avgs_per_day,[out_dir,dx,dy,dat,nb,dom,indx,deps_cells,home,static]); threads.append(p) 
    for indx,dat in enumerate(dates_procc): threads[indx].get()

    print ''; print 'finalizing...'; exp_avg=np.zeros((len(dates),2,7,3)); exp_gridded=np.zeros((len(dates),2,dx*dy)); exp_per_ind=np.zeros((5,2,3,7,nb)); exp_flux=np.zeros((5,2,3,6,3,3))    
    for index_d,day in enumerate(dates):                     
        d=datetime.strptime(day,'%Y-%m-%d'); s=season[d.month]; w=wk[wkends[index_d]]; f=open(out_dir+'/daily_avgs/avgs'+day+'.dat','rb'); avgs=cPickle.load(f); f.close()
        exp_avg[index_d]=avgs['per']; exp_gridded[index_d]=avgs['avg']; exp_per_ind[s+1,w]+=avgs['ungr']; exp_flux[s+1,w]+=avgs['flux']; avgs.clear()   
    
    # for the current period get the number of people exposed in each concentration bin, needed for the bar plot.    
    exp_per_ind[0]=np.sum(exp_per_ind,axis=0); exp_flux[0]=np.sum(exp_flux,axis=0); print ''; pool.close(); print 'calculating binned values for each day...'; pool=Pool(); threads=[]        
    for indx,dat in enumerate(dates_procc): p=pool.apply_async(calc_bins,[out_dir,dat,nb,pop_groups,npg_wday,npg_wend,indx]); threads.append(p) 
    for indx,dat in enumerate(dates_procc): exec("answers%s=threads[indx].get()" %(indx))

    print ''; print 'finalizing...'; exp_per_bin_per_day=[[[[],[]] for _ in range(len(dates))] for _ in range(len(pop_groups1))]; pool.close(); print '' # combine chunks           
    for indx,dat in enumerate(dates_procc):
        for index_d,day in enumerate(dat):                 
            exec("data=answers%s[index_d]" %(indx)); pos=dates.index(day)
            for index_g,g in enumerate(pop_groups1): exp_per_bin_per_day[index_g][pos][0]=data[g][0]; exp_per_bin_per_day[index_g][pos][1]=data[g][1]
      
    # get the number of people exposed in each bin (0.1) from the period and seasonal average exposure concentrations. here i use only the npg_wday file. for the groups related to 
    # a profile proxy e.g., age the nps_wday, npg_wend have the same groups. For other proxies related to habits that change between weekday/weekend e.g., fluxes it is not correct
    for s in range(5): # annual and then the 4 seasons    
        data=[0,0]; T=exp_per_ind[s,0,-1,-1]+exp_per_ind[s,1,-1,-1] # data will store the concentrations based on the Exposure/Time division     
        for p in [0,1]: E=exp_per_ind[s,0,p,-1]+exp_per_ind[s,1,p,-1]; tmp=E/T; inds=np.where(np.isnan(tmp)); tmp[inds]=0.; data[p]=tmp # i change nan values with zero for the total exposure
        for g in pop_groups: exp_per_bin[s][g]=[[],[]]
                        
        if sum(data[1])>0.: # pm2.5 never has zero values therefore i use it in the clause instead of ozone        
           A=post_output_avg(nb,data,pop_groups,npg_wday) # A gives the bins (per 0.1)           
           for g in pop_groups: exp_per_bin[s][g][0]=A[g][0]; exp_per_bin[s][g][1]=A[g][1]           
        else: 
           for g in pop_groups: exp_per_bin[s][g][0],exp_per_bin[s][g][1]=[[] for i in xrange(len(pop_groups))],[[] for i in xrange(len(pop_groups))]            
    
    # this is for the gridded exposure. the exp_gridded has for every cell the exposure for every day of the average of all individuals passing from that cell.
    # the final exp_per_cell dictionary contains the average exposure from all days and individuals for each cell.    
    print 'calculating gridded exposure...'; exp_per_dep=dict(); exp_per_cell=np.zeros((dx,dy,12)) # stores lon,lat,O3 (annual and seasonal=5 values),PM25 (annual and seasonal=5 values)
    for d in [77,78,91,95,75,92,93,94,'PC','GC','P']: exp_per_dep[d]=[[[],[],[],[],[]],[[],[],[],[],[]]]
    
    for day in dates: exp_per_cell_per_day[day]=np.zeros((dx,dy,4))   
    for cell in xrange(dx*dy): 
        ii,jj=get_i_j(cell+1,dx); exp_per_cell[ii-1,jj-1,0]=grid_coords[cell+1][0]; exp_per_cell[ii-1,jj-1,1]=grid_coords[cell+1][1] # cells in grid_coords start from 1!
        c=[[0,0,0,0],[0,0,0,0]]; tmp=np.mean(exp_gridded,axis=0) # this is the gridded exposure of period average for all individuals
        if cell+1 in deps_cells.keys(): dep=deps_cells[cell+1]
        
        for p in [i for i in [0,1] if tmp[i,cell]<>0.]: # don't take into account cells with zero exposure                
            exp_per_cell[ii-1,jj-1,5*p+2]=tmp[p,cell]                
            if cell+1 in deps_cells.keys() and int(dep)>0: exp_per_dep[dep][p][0].append(tmp[p,cell]); exp_per_dep[reg[dep]][p][0].append(tmp[p,cell]) # avg period exposure per department
                                               
        for indx,day in enumerate(dates):
            seas=season[int(day[5:7])]; exp_per_cell_per_day[day][ii-1,jj-1,0]=grid_coords[cell+1][0]; exp_per_cell_per_day[day][ii-1,jj-1,1]=grid_coords[cell+1][1]
            for p in [i for i in [0,1] if exp_gridded[indx,i,cell]<>0.]: # don't take into account cells with zero exposure 
                c[p][seas]+=1; exp_per_cell_per_day[day][ii-1,jj-1,p+2]=exp_gridded[indx,p,cell]; exp_per_cell[ii-1,jj-1,5*p+3+seas]+=exp_gridded[indx,p,cell] # the seasonal values
                if cell+1 in deps_cells.keys() and int(dep)>0: exp_per_dep[dep][p][seas+1].append(exp_gridded[indx,p,cell]); exp_per_dep[reg[dep]][p][seas+1].append(exp_gridded[indx,p,cell])
                     
        for s in [i for i in xrange(4) if exp_per_cell[ii-1,jj-1,3+i]<>0.]: exp_per_cell[ii-1,jj-1,3+s]/=c[0][s] # ozone
        for s in [i for i in xrange(4) if exp_per_cell[ii-1,jj-1,8+i]<>0.]: exp_per_cell[ii-1,jj-1,8+s]/=c[1][s] # pm2.5

    for d in [77,78,91,95,75,92,93,94,'PC','GC','P']:
        for s in [i for i in range(5) if exp_per_dep[d][0][i]<>[]]: exp_per_dep[d][0][s]=round(np.mean(exp_per_dep[d][0][s]),1); exp_per_dep[d][1][s]=round(np.mean(exp_per_dep[d][1][s]),1)
        
    for fl in out_files: f=open(out_dir+'/'+fl+'.dat','wb'); exec("cPickle.dump(%s,f,-1); f.close()" %(fl)); exec("del %s" %(fl)) # output the dat files
            
def get_groups(nb,pp,diary):
        
    npg=dict(); hage_str=['1974','1974','2005','2012','2012']; done=0.; previous=0; P=['75']; PC=['92','93','94']; GC=['77','78','91','95']    
    ac_str=['infants','education','working','unemployed','retired']; age_str=['<4','4-24','25-64','>64']; hage_cat_srt=['cat1','cat2','cat3','cat4','cat5']; oage_cat_str=['cat1','cat2','cat3']
    for g in 'age res ac flux hage hage_cat oage_cat sens'.split(): npg[g]=[0 for i in xrange(nb)]
        
    for k in xrange(nb): # loop all individuals and see how many are exposed to ranges of concentrations (bins of 1)
        age=pp[k][2]; h_age=pp[k][8]; o_age=pp[k][9]; home_dep=str(int(int(pp[k][0])/1000.)); act=pp[k][3]; acts=diary[k]; done+=1; flx=funct(acts); npg['flux'][k]=flx
              
        npg['age'][k]=age_str[age]; npg['ac'][k]=ac_str[act]; npg['hage'][k]=hage_str[h_age-1]; npg['hage_cat'][k]=hage_cat_srt[h_age-1]; npg['oage_cat'][k]=oage_cat_str[o_age-1]
                
        if age==3 and home_dep in GC and h_age==1: npg['sens'][k]='sens1'
        if act==2 and home_dep in P and h_age==1: npg['sens'][k]='sens2'
                
        for ar in ['P','PC','GC']: exec("if home_dep in %s: npg['res'][k]='%s'" %(ar,ar)) # per residence location
        previous=get_progress_bar(done,nb,previous)
                
    return npg          
    
def funct(acts): # it will search the daily itinerary of that individual and get flux information

    deps={'75':'P','92':'PC','93':'PC','94':'PC','77':'GC','78':'GC','91':'GC','95':'GC'}

    for h in xrange(24):               
        for a in xrange(len(acts[h])): 
           time,curr_cell,dest_cell,case,read_IO,curr_loc,dest_loc,mtv=acts[h][a].split(',')
                                           
           if int(mtv)<40: # travelling
              
              # the mtv gives the motive for travelling when the person is doing so and not the motive for the activity in which the individual is heading.    
              if h==23 and len(acts[h])>=a+1: return None                                                
              elif len(acts[h])>a+1: A,B,C,D,E,F,G,H=acts[h][a+1].split(',') # the activity motive is in the next action of the same hour
              else: 
                 A,B,C,D,E,F,G,H=acts[h+1][0].split(',') # the activity motive is in the next hour in the first action
                                                                                       
                 if int(H)<40:                                       
                    try: A,B,C,D,E,F,G,H=acts[h+1][1].split(',') # the activity motive is in the next hour in the second action 
                    except: 
                         try: A,B,C,D,E,F,G,H=acts[h+2][0].split(',') # the activity motive is two hour forwarrd
                         except: return None                        
                         
              if int(H)==61: start,end=curr_loc[0:2],dest_loc[0:2]; return deps[start]+'-->'+deps[end] # people working

def calc_avgs_per_day(out_dir,dx,dy,dates,nb,dom,index_chunk,deps_cells,home,static): # calculates for each individual the gridded average (avgs['ungr']) and for every cell the individuals average(avgs['avg'])

    from numpy import percentile as pctl
        
    done=0.; previous=0; reg={75:0,92:1,93:1,94:1,77:2,78:2,91:2,95:2,0:0}; np.seterr(invalid='ignore',divide='ignore'); max_l=6; ls=range(7)
    if static==1: max_l==1; ls=[0,6]    
    
    for indx_d,day in enumerate(dates):
            
        if indx_d==0 and index_chunk==0: print '0% completed.'       
        if isfile(out_dir+'/daily_avgs/avgs'+day+'.dat') and index_chunk==0: done+=1.; previous=get_progress_bar(done,len(dates),previous)
        if isfile(out_dir+'/daily_avgs/avgs'+day+'.dat'): continue
        
        f=open(out_dir+'/exp_'+dom+'_'+day+'.dat','rb'); exp=cPickle.load(f); f.close(); tmp=[[[[] for _ in range(6)] for _ in xrange(nb)] for _ in range(3)]
        avgs=dict(); avgs['per']=np.zeros((2,7,3)); avgs['avg']=np.zeros((2,dx*dy)); avgs['ungr']=np.zeros((3,7,nb)) # 3 is o3,pm2.5,time and 7 is to store the exposure modes + the total        
        avgs['flux']=np.zeros((3,6,3,3)) # pol+time,6 exposure modes,residence area (0:P,1:PC,2:GC),where the exposure took place (0:P,1:PC,2:GC)
         
        for cell in exp.keys(): # for each grid cell average all individuals      
            end=reg[int(deps_cells[cell])]; ar=[[],[],[]] # 2 pollutants and time
                                    
            for k in exp[cell].keys(): # some individuals will not be in any cell. these are the people that stay in some border cells of idf which are not considered for the calculation                                          
                for l in range(max_l):                
                    for p in [0,1,2]: ar[p].append(exp[cell][k][p][l]); tmp[p][k][l].append(ar[p][-1]); avgs['flux'][p,l,home[k],end]+=ar[p][-1] # appends all values of o3,pm2.5,time of individuals for the current cell
                                                                                
            time=float(sum(np.hstack(ar[2])))/2. # the time saved inside exp files is double the actual time
            for p in [0,1]: conc=np.hstack(ar[p]); avgs['avg'][p][cell-1]=round(float(sum(conc))/time,1)
                
        exp.clear()
                
        # populations average (for all grid cells)
        for k in xrange(nb): # sums exposure and time for each individual and in each of the 6 modes
            for l in range(max_l): avgs['ungr'][0,l,k]=sum(tmp[0][k][l]); avgs['ungr'][1,l,k]=sum(tmp[1][k][l]); avgs['ungr'][2,l,k]=sum(tmp[2][k][l])/2.
                            
        avgs['ungr'][:,-1,:]=(avgs['ungr']).sum(axis=1) # sums exposure and time in all modes (its for the total exposure concentration)
                                                
        for l in ls: 
            T=avgs['ungr'][-1,l]
            for p in [0,1]: E=avgs['ungr'][p,l]; C_exp=E/T; tmp=C_exp[~np.isnan(C_exp)]; avgs['per'][p,l,0]=np.mean(tmp); avgs['per'][p,l,1]=pctl(tmp,5); avgs['per'][p,l,2]=pctl(tmp,95)
                                                                
        of=open(out_dir+'/daily_avgs/avgs'+day+'.dat','wb'); cPickle.dump(avgs,of,-1); of.close(); tmp=[]; avgs['per'],avgs['avg'],avgs['ungr'],avgs['flux']=None,None,None,None
                
        if index_chunk==0: done+=1.; previous=get_progress_bar(done,len(dates),previous)                
        
def calc_bins(out_dir,dates,nb,pop_groups,npg_wday,npg_wend,index_chunk): # this is for the daily only
            
    bins=dict(); bins=defaultdict(dict); done=0.; previous=0; pol=['o3','pm25']; wkd={0:'wday',1:'wday',2:'wday',3:'wday',4:'wday',5:'wend',6:'wend'}; weekends=get_day_of_week(dates)
    np.seterr(invalid='ignore'); step=0.1
    
    for indx_d,day in enumerate(dates): 
                
        if indx_d==0 and index_chunk==0: print '0% completed.'   
                                
        w=wkd[weekends[indx_d]]; f=open(out_dir+'/daily_avgs/avgs'+day+'.dat','rb'); avgs=cPickle.load(f); f.close(); data=avgs['ungr']; avgs.clear(); exec("npg=npg_%s" %(w)); dt=[0,0]; T=data[-1,-1]
        for p in [0,1]: C=data[p,-1]; tmp=C/T; inds=np.where(np.isnan(tmp)); tmp[inds]=0.; dt[p]=tmp # calculate conc exposure for each ind (i put the invalid ind in conc 0)
    
        max_r_o3=int(round(max(dt[0]),1)/step)+1; max_r_pm25=int(round(max(dt[1]),1)/step)+2; indx=[0,0]; cs=[np.array([step*i for i in xrange(max_r_o3)]),np.array([step*i for i in xrange(max_r_pm25)])]
        o3=np.zeros((len(pop_groups),len(cs[0]))); pm25=np.zeros((len(pop_groups),len(cs[1])))        
                
        for i,p in enumerate(pol): exec("indx[%s]=srch(cs[%s],dt[%s],side='right')-1" %(i,i,i)); exec('%s_=cnt(indx[%s])' %(p,i)); exec('for j in %s_.keys(): %s[0,j]=%s_[j]' %(p,p,p))                
                
        for k in xrange(len(indx[0])): # might not have all the individuals; these are the people that always stay in some border cell of idf which is not considered for the calculation 
            for gr in ['age','res','ac','hage']: id=pop_groups.index(npg[gr][k]); o3[id,indx[0][k]]+=1; pm25[id,indx[1][k]]+=1                                                               
            for gr in ['flux']: 
                try: id=pop_groups.index(npg[gr][k]); o3[id,indx[0][k]]+=1; pm25[id,indx[1][k]]+=1
                except: pass
                        
        for index_g,g in enumerate(pop_groups): bins[indx_d][g]=[[],[]]; bins[indx_d][g][0]=o3[index_g]; bins[indx_d][g][1]=pm25[index_g]
        
        if index_chunk==0: done+=1.; previous=get_progress_bar(done,len(dates),previous)   
    
    return bins

def post_output_avg(nb,dt,pop_groups,npg_wday):
                
    step=0.1; bins=dict(); pol=['o3','pm25']; npg=npg_wday; max_r_o3=int(round(max(dt[0]),1)/step)+1; max_r_pm25=int(round(max(dt[1]),1)/step)+1; indx=[0,0] # dt here is exp_per_ind
    cs=[np.array([step*i for i in xrange(max_r_o3)]),np.array([step*i for i in xrange(max_r_pm25)])]; o3=np.zeros((len(pop_groups),len(cs[0]))); pm25=np.zeros((len(pop_groups),len(cs[1])))
        
    for i,p in enumerate(pol): exec("indx[%s]=srch(cs[%s],dt[%s],side='right')-1" %(i,i,i)); exec('%s_=cnt(indx[%s])' %(p,i)); exec('for j in %s_.keys(): %s[0,j]=%s_[j]' %(p,p,p))
        
    for k in xrange(len(indx[0])): # might not have all the individuals; these are the people that always stay in some border cell of idf which is not considered for the calculation 
        for gr in ['age','res','ac','hage']: id=pop_groups.index(npg[gr][k]); o3[id,indx[0][k]]+=1; pm25[id,indx[1][k]]+=1
        for gr in ['flux']: 
            try: id=pop_groups.index(npg[gr][k]); o3[id,indx[0][k]]+=1; pm25[id,indx[1][k]]+=1
            except: pass
            
    for index_g,g in enumerate(pop_groups): bins[g]=[[],[]]; bins[g][0]=o3[index_g]; bins[g][1]=pm25[index_g]
            
    return bins
                                      
def get_i_j(curr_id,dx):

    jj=int(int(curr_id)/dx) + 1; ii = int(curr_id) - dx*(jj-1)    
    if ii==0: ii=dx; jj-=1 # Because the above func in line=39,78... will give i=0 (wrong!)
    
    return ii,jj

def get_avg_conc(dates,path_to_conc,conc_dir,dx,dy,grid_coords,idf_cells,dom,chim_label):
	
    print 'generating gridded period and seasonal CHIMERE concentration average for the selected period...'; done=0; previous=0; print '0% completed.'
    conc_avg=np.zeros((dx,dy,12)); tmp0=dict(); tmp1=dict(); season={1:0,2:0,12:0,3:1,4:1,5:1,6:2,7:2,8:2,9:3,10:3,11:3}; count=[0,0,0,0]; conc_per_dep=dict(); c_in_deps=dict()
    of=open('domains/'+dom+'/deps_cells.dat','rb'); deps_cells=cPickle.load(of); of.close(); reg={92:'PC',93:'PC',94:'PC',77:'GC',78:'GC',91:'GC',95:'GC',75:'P'}
    tm_per_dep=[[[],[],[],[],[],[],[],[],[]],[[],[],[],[],[],[],[],[],[]]]; deps=[75,92,93,94,77,78,91,95]
    
    for d in [77,78,91,95,75,92,93,94,'PC','GC','P']: conc_per_dep[d]=[[[],[],[],[],[]],[[],[],[],[],[]]]; c_in_deps[d]=[c-1 for c in deps_cells.keys() if deps_cells[c]==d]
        
    for index_d,day in enumerate(dates): # these are the days

        try: f=open(conc_dir+'/conc_'+day+'.dat','rb'); conc=cPickle.load(f); f.close(); seas=season[int(day[5:7])]+1; count[seas-1]+=1
        except: print 'there are no concetration files for the requested dates in '+conc_dir+'. check the conc_label flag in expo.par.'; sys.exit(0)
        
        for s in [i for i in range(5) if index_d==0]: tmp0[s]=np.zeros_like(conc['O3'][0]); tmp1[s]=np.zeros_like(conc['PM25'][0])

        o3,pm=np.mean(conc['O3'],axis=0),np.mean(conc['PM25'],axis=0); tmp0[0]+=o3; tmp1[0]+=pm; tmp0[seas]+=o3; tmp1[seas]+=pm
                
        for index_d,d in enumerate(deps): tm_per_dep[0][index_d].append(np.mean(o3[c_in_deps[d]])); tm_per_dep[1][index_d].append(np.mean(pm[c_in_deps[d]]))
                
        tm_per_dep[0][-1].append(np.mean(conc['O3'])); tm_per_dep[1][-1].append(np.mean(conc['PM25'])) # time-series of domain average conc
        
        done+=1.; previous=get_progress_bar(done,len(dates),previous)
                
    conc.clear(); tmp0[0],tmp1[0]=tmp0[0]/len(dates),tmp1[0]/len(dates)
    for s in [i for i in [1,2,3,4] if count[i-1]>0]: tmp0[s],tmp1[s]=tmp0[s]/count[s-1],tmp1[s]/count[s-1]
        
    for cell in xrange(dx*dy): # per department
        ii,jj=get_i_j(cell+1,dx); conc_avg[ii-1,jj-1,0]=grid_coords[cell+1][0]; conc_avg[ii-1,jj-1,1]=grid_coords[cell+1][1] # the lats,lons
        
        if cell+1 in idf_cells: 
           dep=deps_cells[cell+1]; conc_avg[ii-1,jj-1,2]=tmp0[0][cell]; conc_avg[ii-1,jj-1,7]=tmp1[0][cell]                     
           if int(dep)>0: conc_per_dep[dep][0][0].append(tmp0[0][cell]); conc_per_dep[reg[dep]][0][0].append(tmp0[0][cell])
           if int(dep)>0: conc_per_dep[dep][1][0].append(tmp1[0][cell]); conc_per_dep[reg[dep]][1][0].append(tmp1[0][cell])
           
           for s in [i for i in [1,2,3,4] if count[i-1]>0]: 
               conc_avg[ii-1,jj-1,2+s]=tmp0[s][cell]; conc_avg[ii-1,jj-1,7+s]=tmp1[s][cell]        
               if int(dep)>0: conc_per_dep[dep][0][s].append(tmp0[s][cell]); conc_per_dep[reg[dep]][0][s].append(tmp0[s][cell])
               if int(dep)>0: conc_per_dep[dep][1][s].append(tmp1[s][cell]); conc_per_dep[reg[dep]][1][s].append(tmp1[s][cell])

    for d in [77,78,91,95,75,92,93,94,'PC','GC','P']:
        for s in [i for i in range(5) if conc_per_dep[d][0][i]<>[]]: conc_per_dep[d][0][s]=round(np.mean(conc_per_dep[d][0][s]),1); conc_per_dep[d][1][s]=round(np.mean(conc_per_dep[d][1][s]),1)
        for s in [i for i in range(5) if conc_per_dep[d][0][i]==[]]: conc_per_dep[d][0][s]=0; conc_per_dep[d][1][s]=0
    
    f=open(path_to_conc+'/'+chim_label+'/conc_avg_'+dates[0]+'_'+dates[-1]+'.txt','w'); wrt=str(len(dates))+' days ('+dates[0]+' to '+dates[-1]+')'; print >> f,wrt
    for indx,pol in enumerate('O3 PM25'.split()): 
        print >> f,''
        for d in [75,92,93,94,77,78,91,95,'PC','GC']: 
            wrt='in '+str(d)+': '
            for s in range(5): wrt+=str(conc_per_dep[d][indx][s])+'  '
            print >> f,wrt
        
    of=open(path_to_conc+'/'+chim_label+'/conc_avg_'+dates[0]+'_'+dates[-1]+'.dat','wb'); cPickle.dump(conc_avg,of,-1); of.close(); f.close()
    of=open(path_to_conc+'/'+chim_label+'/conc_avg_per_day_'+dates[0]+'_'+dates[-1]+'.dat','wb'); cPickle.dump(tm_per_dep,of,-1); of.close(); f.close()
        
def get_progress_bar(done,ln,previous): # count how many individuals were completed
        
    j=(done/float(ln))*100.; i,d=divmod(j,1)      
    if i>previous: previous=i; print str(int(i))+'% completed.'
               
    return previous 
    
def selection(perc_list,write_id):

    sum_p=0.; id_selection=0; sorted_list=[0.]           
    
    for index_p,p in enumerate(perc_list): sum_p+=p; sorted_list.append(sum_p) #in index_p=0, sum_p is zero and it adds this as the 1st element of sorted_list
        
    if len(sorted_list)>2 and 0.999 < sum(sorted_list) < 1.001: sorted_list[-1]=1. # Some times the sum gives a slightly lower than 1 number (e.g. 0.9999999998)
    elif sorted_list[-1]<>1.: sorted_list.append(1.) # if the array has only 0. and one % value then the above if will mistakenly remove the % and place an 1.
        
    tau = uniform(0,1)                    
    for index_p,p in enumerate(sorted_list[0:-1]): # The last item is not iterated its p=1.0           
        if tau>=p and tau<sorted_list[index_p+1]: id_selection=index_p+write_id; break
        
    return id_selection   
